{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from fuzzywuzzy import fuzz\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import distance\n",
    "import time\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, precision_recall_curve, precision_score, recall_score\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import KMeansSMOTE, BorderlineSMOTE, SVMSMOTE\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import spacy\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "test_labels = pd.read_csv('../input/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and extracting features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ip addresses\n",
    "def ip_add(string):\n",
    "    text=re.findall('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}',string)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mac addresses\n",
    "def mac_add(string):\n",
    "    text=re.findall('(?:[0-9a-fA-F]:?){12}',string)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the results in 4 decemal points\n",
    "SAFE_DIV = 0.0001 \n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = text.replace(\",000,000\", \"m\").replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\",000\", \"k\").replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"′\", \"'\").replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\n",
    "    text = text.replace(\"’\", \"'\").replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\n",
    "    text = text.replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"€\", \" euro \").replace(\"'ll\", \" will\")                       \n",
    "    text = re.sub(r\"([0-9]+)000000\", r\"\\1m\", text)\n",
    "    text = re.sub(r\"([0-9]+)000\", r\"\\1k\", text)\n",
    "    \n",
    "    \n",
    "    porter = SnowballStemmer(language='english')\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(text) == type(''):\n",
    "        text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    \n",
    "    if type(text) == type(''):\n",
    "        text = porter.stem(text)\n",
    "        example1 = BeautifulSoup(text)\n",
    "        text = example1.get_text()\n",
    "                   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['num_mac'] = [len(i) for i in train_df['comment_text'].apply(lambda x: mac_add(x))]\n",
    "train_df['num_ip'] = [len(i) for i in train_df['comment_text'].apply(lambda x: ip_add(x))]\n",
    "test_df['num_mac'] = [len(i) for i in test_df['comment_text'].apply(lambda x: mac_add(x))]\n",
    "test_df['num_ip'] = [len(i) for i in test_df['comment_text'].apply(lambda x: ip_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_comments'] = train_df['comment_text'].apply(lambda x : preprocess(x))\n",
    "test_df['processed_comments'] = test_df['comment_text'].apply(lambda x : preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(sent):\n",
    "             \n",
    "    sent = str(sent)\n",
    "    final_sent = []\n",
    "    for word in sent.split(' '):\n",
    "        if len(word) > 2:\n",
    "            final_sent.append(word)\n",
    "        \n",
    "    sent = ' '.join(final_sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['final_processed_comments'] = train_df['processed_comments'].apply(lambda x : final(x))\n",
    "test_df['final_processed_comments'] = test_df['processed_comments'].apply(lambda x : final(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = train_df\n",
    "final_test = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting word and comment lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train['comment_length'] = [len(i) for i in final_train['final_processed_comments']]\n",
    "final_test['comment_length'] = [len(i) for i in final_test['final_processed_comments']]\n",
    "final_train['num_words'] = final_train['final_processed_comments'].apply(lambda x: len(list(str(x).strip().split(' '))))\n",
    "final_test['num_words'] = final_test['final_processed_comments'].apply(lambda x: len(list(str(x).strip().split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_csv('final_train.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.to_csv('final_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free some memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del(final_train)\n",
    "    del(final_test)\n",
    "    del(train_df)\n",
    "    del(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-2 Data Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will use tfidf weighted Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('final_train.csv')\n",
    "test_df = pd.read_csv('final_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['final_processed_comments'].fillna(value = 'neutral', inplace=True)\n",
    "test_df['final_processed_comments'].fillna(value='neutral', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train = train_df['final_processed_comments']\n",
    "comments_test = test_df['final_processed_comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False, )\n",
    "tfidf.fit_transform(comments_train)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vectors_web_lg, which includes over 1 million unique vectors.\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train = []\n",
    "vec_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is used to print the progress bar\n",
    "\n",
    "for text in tqdm(list(train_df['final_processed_comments']), total=len(list(train_df['final_processed_comments']))):\n",
    "    doc_train = nlp(text) \n",
    "    # 384 is the number of dimensions of vectors \n",
    "    mean_vec_train = np.zeros([len(doc_train), len(doc_train[0].vector)])\n",
    "    for word in doc_train:\n",
    "        # word2vec\n",
    "        vec_train = word.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word)]\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec_trian += vec_train * idf\n",
    "    mean_vec_trian = mean_vec_trian.mean(axis=0)\n",
    "    vecs1.append(mean_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['final_features'] = list(vecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = ['comment_text', 'processed_comments', 'final_processed_comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in tqdm(list(test_df['final_processed_comments'])):\n",
    "    doc2 = nlp(text) \n",
    "    mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n",
    "    for word2 in doc2:\n",
    "        # word2vec\n",
    "        vec2 = word2.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word2)]\n",
    "        except:\n",
    "            #print word\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec2 += vec2 * idf\n",
    "    mean_vec2 = mean_vec2.mean(axis=0)\n",
    "    vecs2.append(mean_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['final_features'] = list(vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns = ['comment_text', 'processed_comments', 'final_processed_comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.DataFrame(train_df.final_features.tolist(), index=train_df.id)\n",
    "new_test_df = pd.DataFrame(test_df.final_features.tolist(), index=test_df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = final_train_df.merge(new_df, on='id', how='left')\n",
    "final_test_df = final_df_test.merge(new_test_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting vactorized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df.to_csv('final_train.csv')\n",
    "final_test_df.to_csv('final_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del(final_train_df)\n",
    "    del(final_test_df)\n",
    "    del(train_df)\n",
    "    del(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-3 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('final_train_df.csv')\n",
    "test_df = pd.read_csv('final_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting data to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert all the features into numaric before we apply any model\n",
    "cols = list(train_df.columns)[1:]\n",
    "for i in tqdm(cols):\n",
    "    train_df[i] = train_df[i].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert all the features into numaric before we apply any model\n",
    "cols = list(test_df.columns)[1:]\n",
    "for i in tqdm(cols):\n",
    "    test_df[i] = test_df[i].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To plot confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the confusion matrices given y_i, y_i_hat.\n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    #divid each element of the confusion matrix with the sum of elements in that column\n",
    "    \n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.T = [[1, 3],\n",
    "    #        [2, 4]]\n",
    "    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =1) = [[3, 7]]\n",
    "    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n",
    "    #                           [2/3, 4/7]]\n",
    "\n",
    "    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n",
    "    #                           [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    #divid each element of the confusion matrix with the sum of elements in that row\n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =0) = [[4, 6]]\n",
    "    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n",
    "    #                      [3/4, 4/6]] \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = [1,2]\n",
    "    # representing A in heatmap format\n",
    "    cmap=sns.light_palette(\"blue\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_df.drop(columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting test_df data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_features = test_df.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count,label in enumerate(train_labels.columns):\n",
    "    \n",
    "    print('Resampling {}.....'.format(label))\n",
    "    x_resampled, y_resampled = BorderlineSMOTE(n_jobs=8).fit_resample(x_train, y_train[label])\n",
    "    \n",
    "    if count==0:\n",
    "        x_resampled[label] = y_resampled\n",
    "        x_final = x_resampled\n",
    "        print(x_final.shape)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        x_resampled[label] = y_resampled\n",
    "        x_resampled = x_resampled[x_resampled[label] == 1]\n",
    "        x_final[label] = 0\n",
    "        x_final = pd.concat([x_final, x_resampled], ignore_index=False)\n",
    "        print(x_final.shape)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_labels.columns:\n",
    "    print(x_final[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_labels.columns:\n",
    "    print(x_final[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the x_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final.to_csv('final_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Chains - A xgboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_chains = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to add features\n",
    "def add_feature(X, feature_to_add):\n",
    "    '''\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    '''\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in train_labels.columns:\n",
    "    \n",
    "    print('Resampling {}.....\\n'.format(label))\n",
    "    x_resampled, y_resampled = BorderlineSMOTE(n_jobs=8).fit_resample(x_train, y_train[label])\n",
    "    \n",
    "    \n",
    "    print('... Processing {}'.format(label))\n",
    "    y_train_label = y_resampled\n",
    "    y_test_label = y_test[label]\n",
    "    # train the model using x_train & y_train_label\n",
    "    xgb.fit(x_resampled,y_train_label)\n",
    "    \n",
    "    # compute the training accuracy\n",
    "    y_pred_X = xgb.predict(x_resampled)\n",
    "    print('Training Accuracy is {}'.format(accuracy_score(y_train_label,y_pred_X)))\n",
    "    \n",
    "    # make predictions from x_test and compute test accuracy\n",
    "    y_test_pred = xgb.predict(x_test)\n",
    "    print('Test Accuracy is {}'.format(accuracy_score(y_test_label, y_test_pred)))\n",
    "    y_test_prob = xgb.predict_proba(x_test)[:,1]\n",
    "        \n",
    "    # chain current label to x_train\n",
    "    x_train = add_feature(x_train, y_train[label])\n",
    "    print('Shape of x_train is now {}'.format(x_train.shape))\n",
    "    \n",
    "    # chain current label predictions to x_test\n",
    "    x_test = add_feature(x_test, y_test_label)\n",
    "    print('Shape of x_test is now {}'.format(x_test.shape))\n",
    "    \n",
    "    print('Train Confusion Matrix:')\n",
    "    plot_confusion_matrix(y_train_label, y_pred_X)\n",
    "    print('\\n Test Confusion Matrix:')\n",
    "    plot_confusion_matrix(y_test_label, y_test_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Making submission file\n",
    "    test_df_pred = xgb.predict(test_df_features)\n",
    "    test_df_prob = xgb.predict_proba(test_df_features)[:,1]\n",
    "    submission_chains[label] = test_df_prob\n",
    "    test_df_features = add_feature(test_df_features, test_df_prob)\n",
    "    print('Shape of test_df is now {}'.format(test_df.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
